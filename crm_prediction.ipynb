{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import visualizer as viz\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'\n",
    "features_raw = pd.read_csv(path + 'orange_small_train.data', sep = '\\t')\n",
    "features_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = features_raw.iloc[:, 0:190].copy()\n",
    "categorical_features = features_raw.iloc[:, 190:].copy()\n",
    "\n",
    "print('Shape of numerical features: {}\\nShape of categorical features: {}'.format(numerical_features.shape, categorical_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = features_raw.iloc[:, 170:200].copy()\n",
    "viz.plot_missing_matrix(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_missing_bar(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "![features](https://media.giphy.com/media/c6J4HiJ8aNRSBrMYfH/giphy.gif)\n",
    "Nem toda feature tem o mesmo impacto no modelo, por isso, vamos tentar descobrir quais features têm o maior impacto na predição.\n",
    "Vamos fazer isso em 2 passos:\n",
    "\n",
    "1. Remover variáveis com base no número valores distintos\n",
    "2. Permutações de importância"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As variáveis não existirem pode ser preditivo, mas vamos ignorar isso por enquanto e preencher o vazio com a média da instancia para features numericas e com a string 'missing' para as features categóricas (isso já é suficiente para predizer através do vazio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina as features que possuem o mesmo valor para todas as intâncias ou são exclusivamente nulas\n",
    "categorical_features = utils.drop_min_unique_features(categorical_features, 1)\n",
    "\n",
    "print(categorical_features.shape)\n",
    "viz.plot_missing_matrix(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Substitui vazio por 'missing'\n",
    "categorical_features.fillna('missing', inplace=True)\n",
    "\n",
    "categorical_features.astype('category', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina as features que possuem o mesmo valor para todas as intâncias ou são exclusivamente nulas\n",
    "numerical_features = utils.drop_min_unique_features(numerical_features, 1)\n",
    "\n",
    "numerical_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Substitui vazio pela média\n",
    "numerical_features.fillna(numerical_features.mean(), inplace=True)\n",
    "\n",
    "numerical_features.astype('float', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categories_per_feature(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = utils.drop_max_unique_features(categorical_features, 6000)\n",
    "viz.plot_categories_per_feature(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat((numerical_features, categorical_features), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria uma cópia do dataset para predicao do churn\n",
    "Calcula a importancia das features (quais features realmente impactam a predição)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_features = pd.get_dummies(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_labels = pd.read_csv(path + 'orange_small_train_churn.labels', header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutações\n",
    "Vamos \"embaralhar\" os valores de uma coluna de cada vez e avaliar quanto aquela coluna (isoladamente) afeta nossas predições.\n",
    "Os valores que tiverem o maior peso representam as features mais importantes e os de menos peso, as menos importantes.\n",
    "\n",
    "É como se em um jogo de paciência embaralhamos cada fila de cartas e medimos como isso afeta nosso resultado final, depois voltamos ao estado original e embaralhamos a próxima fila e assim por diante até termos embaralhado e medido todas as filas. (Para diminuir a influência do acaso, fazemos esse processo várias vezes)\n",
    "![solitaire](https://www.hajapaciencia.com.br/static/main/thumbs/paciencia-canadense-1.df4d06d88fe1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_features = utils.permutation_importance(churn_features, churn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a small subset (20%) just to check the classifiers\n",
    "_, X_train, _, y_train = utils.split_dataset(churn_features, churn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = utils.get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.train_and_report(models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que sabemos o melhor modelo para a tarefa, vamos otimizar alguns parametros. Utilizar grid search p/ otimizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.array([])\n",
    "opt_params = dict()\n",
    "scores = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "n_estimators = None\n",
    "max_depth = 8\n",
    "min_samples_split = 250\n",
    "min_samples_leaf = 20\n",
    "max_features = 'sqrt'\n",
    "subsample = 0.8\n",
    "params = {'n_estimators': range(50, 151, 10)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = opt_params['n_estimators']\n",
    "max_depth = None\n",
    "min_samples_split = None\n",
    "min_samples_leaf = 20\n",
    "max_features = 'sqrt'\n",
    "subsample = 0.8\n",
    "params = {'max_depth': range(3, 12, 2), 'min_samples_split': range(150, 401, 50)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = opt_params['max_depth']\n",
    "min_samples_split = opt_params['min_samples_split']\n",
    "min_samples_leaf = None\n",
    "max_features = 'sqrt'\n",
    "subsample = 0.8\n",
    "params = {'min_samples_leaf': range(25, 61, 5)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf = opt_params['min_samples_leaf']\n",
    "max_features = None\n",
    "subsample = 0.8\n",
    "params = {'max_features': range(21, 31, 1)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = opt_params['max_features']\n",
    "subsample = None\n",
    "params = {'subsample': np.append(np.arange(0.6, 1, 0.05), 1)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = opt_params['subsample']\n",
    "learning_rate = 0.1\n",
    "n_estimators = opt_params['n_estimators']\n",
    "\n",
    "models, scores = utils.gbc_lr_optimizer(n_estimators = n_estimators,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        min_samples_split = min_samples_split,\n",
    "                                        min_samples_leaf = min_samples_leaf,\n",
    "                                        max_depth = max_depth,\n",
    "                                        max_features = max_features,\n",
    "                                        subsample = subsample,\n",
    "                                        params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, models)\n",
    "scores = np.append(scores, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best performance: {} | Model: {}'.format(max(scores), scores.argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(churn_features, churn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_model = models[scores.argmax()]\n",
    "churn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_model(churn_model, 'final_churn_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_predictions = churn_model.predict_proba(X_test)\n",
    "roc_auc_score(y_test, churn_predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appetency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appetency_features = pd.get_dummies(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appetency_labels = pd.read_csv(path + 'orange_small_train_appetency.labels', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appetency_features = utils.permutation_importance(appetency_features, appetency_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appetency_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a small subset (20%) just to check the classifiers\n",
    "_, X_train, _, y_train = utils.split_dataset(appetency_features, appetency_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = utils.get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.train_and_report(models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que sabemos o melhor modelo para a tarefa, vamos otimizar alguns parametros. Utilizar grid search p/ otimizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.array([])\n",
    "opt_params = dict()\n",
    "scores = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "n_estimators = None\n",
    "max_depth = 8\n",
    "min_samples_split = 250\n",
    "min_samples_leaf = 20\n",
    "max_features = 'sqrt'\n",
    "subsample = 0.8\n",
    "params = {'n_estimators': range(50, 151, 10)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = opt_params['n_estimators']\n",
    "max_depth = None\n",
    "min_samples_split = None\n",
    "min_samples_leaf = 20\n",
    "max_features = 'sqrt'\n",
    "subsample = 0.8\n",
    "params = {'max_depth': range(3, 12, 2), 'min_samples_split': range(150, 401, 50)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = opt_params['max_depth']\n",
    "min_samples_split = opt_params['min_samples_split']\n",
    "min_samples_leaf = None\n",
    "max_features = 'sqrt'\n",
    "subsample = 0.8\n",
    "params = {'min_samples_leaf': range(25, 61, 5)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf = opt_params['min_samples_leaf']\n",
    "max_features = None\n",
    "subsample = 0.8\n",
    "params = {'max_features': range(21, 31, 1)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = opt_params['max_features']\n",
    "subsample = None\n",
    "params = {'subsample': np.append(np.arange(0.6, 1, 0.05), 1)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = opt_params['subsample']\n",
    "learning_rate = 0.1\n",
    "n_estimators = opt_params['n_estimators']\n",
    "\n",
    "models, scores = utils.gbc_lr_optimizer(n_estimators = n_estimators,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        min_samples_split = min_samples_split,\n",
    "                                        min_samples_leaf = min_samples_leaf,\n",
    "                                        max_depth = max_depth,\n",
    "                                        max_features = max_features,\n",
    "                                        subsample = subsample,\n",
    "                                        params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, models)\n",
    "scores = np.append(scores, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best performance: {} | Model: {}'.format(max(scores), scores.argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(appetency_features, appetency_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appetency_model = models[scores.argmax()]\n",
    "appetency_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_model(appetency_model, 'final_appetency_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appetency_predictions = appetency_model.predict_proba(X_test)\n",
    "roc_auc_score(y_test, appetency_predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upselling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upselling_features = pd.get_dummies(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upselling_labels = pd.read_csv(path + 'orange_small_train_upselling.labels', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upselling_features = utils.permutation_importance(upselling_features, upselling_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upselling_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a small subset (20%) just to check the classifiers\n",
    "_, X_train, _, y_train = utils.split_dataset(upselling_features, upselling_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = utils.get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.train_and_report(models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que sabemos o melhor modelo para a tarefa, vamos otimizar alguns parametros. Utilizar grid search p/ otimizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.array([])\n",
    "opt_params = dict()\n",
    "scores = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "n_estimators = None\n",
    "max_depth = 8\n",
    "min_samples_split = 250\n",
    "min_samples_leaf = 20\n",
    "max_features = 'sqrt'\n",
    "subsample = 0.8\n",
    "params = {'n_estimators': range(50, 151, 10)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = opt_params['n_estimators']\n",
    "max_depth = None\n",
    "min_samples_split = None\n",
    "min_samples_leaf = 20\n",
    "max_features = 'sqrt'\n",
    "subsample = 0.8\n",
    "params = {'max_depth': range(3, 12, 2), 'min_samples_split': range(150, 401, 50)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = opt_params['max_depth']\n",
    "min_samples_split = opt_params['min_samples_split']\n",
    "min_samples_leaf = None\n",
    "max_features = 'sqrt'\n",
    "subsample = 0.8\n",
    "params = {'min_samples_leaf': range(25, 61, 5)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf = opt_params['min_samples_leaf']\n",
    "max_features = None\n",
    "subsample = 0.8\n",
    "params = {'max_features': range(21, 31, 1)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = opt_params['max_features']\n",
    "subsample = None\n",
    "params = {'subsample': np.append(np.arange(0.6, 1, 0.05), 1)}\n",
    "\n",
    "model, opt_param, score = utils.gbc_params_optimizer(n_estimators = n_estimators,\n",
    "                                               learning_rate = learning_rate,\n",
    "                                               min_samples_split = min_samples_split,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               max_depth = max_depth,\n",
    "                                               max_features = max_features,\n",
    "                                               subsample = subsample,\n",
    "                                               params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, model)\n",
    "opt_params = {**opt_params, **opt_param}\n",
    "scores = np.append(scores, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = opt_params['subsample']\n",
    "learning_rate = 0.1\n",
    "n_estimators = opt_params['n_estimators']\n",
    "\n",
    "models, scores = utils.gbc_lr_optimizer(n_estimators = n_estimators,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        min_samples_split = min_samples_split,\n",
    "                                        min_samples_leaf = min_samples_leaf,\n",
    "                                        max_depth = max_depth,\n",
    "                                        max_features = max_features,\n",
    "                                        subsample = subsample,\n",
    "                                        params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.append(models, models)\n",
    "scores = np.append(scores, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best performance: {} | Model: {}'.format(max(scores), scores.argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(upselling_features, upselling_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upselling_model = models[scores.argmax()]\n",
    "upselling_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_model(upselling_model, 'final_upselling_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upselling_predictions = upselling_model.predict_proba(X_test)\n",
    "roc_auc_score(y_test, upselling_predictions[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "Obtivemos uma predição bla bla, teriamos ficado em bla.\n",
    "\n",
    "### O que poderia ter sido melhor?\n",
    "- O dataset é anonimizado e as variáveis são renomeadas para `var_n` por tanto não sabemos a que se refere cada variável. Poderíamos obter um melhor resultado se soubéssemos do que se trata cada uma, além de nos permitir ter uma visão de negócios sobre o algoritmo que estamos usando. Por exemplo, poderíamos calcular os _SHAP values_ de diferentes variáveis para entender como cada uma afeta a experiência do cliente e sua chance de churnar, fazer um upsell ou comprar novos produtos.\n",
    "    - Digamos que a variável BLA se refere ao NPS (Net Promoter Score, uma forma de medir a chance de um cliente te recomendar a um amigo), plotando o gráfico dos valores SHAP vemos que claramente um valor baixo de NPS está relacionado com churn. Podemos então fazer um estudo mais aprofundado para o melhoramento do nosso atendimento ao cliente, aumentando o NPS e, por consequencia, diminuindo o churn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
